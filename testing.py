# -*- coding: utf-8 -*-
"""14_1_25_one_script (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FlutXKJeChjKUA1twlNyPlbUXb3l521a
"""

import cv2
from ultralytics import YOLO
import gradio as gr
import os
# import easyocr
import csv
from deep_sort_realtime.deepsort_tracker import DeepSort
from paddleocr import PaddleOCR
import time 
from datetime import datetime
import numpy as np
import sys
import webcolors

device = "cuda"

def calculate_overlap_percentage(car_bbox, parking_lot_mask):
    x1, y1, x2, y2 = map(int, car_bbox)
    car_roi = parking_lot_mask[y1:y2, x1:x2]
    car_area = (x2 - x1) * (y2 - y1)
    overlap_area = np.sum(car_roi)
    overlap_percentage = (overlap_area / car_area) * 100 if car_area > 0 else 0
    return overlap_percentage


def is_car_in_multiple_parking_lots(car_bbox, parking_lot_masks, threshold=12):
    overlap_count = 0
    for parking_lot_mask in parking_lot_masks:
        overlap_percentage = calculate_overlap_percentage(car_bbox, parking_lot_mask)
        if overlap_percentage > threshold:
            overlap_count += 1
    return overlap_count


def detected_car_color(image):
    pixels = image.reshape(-1, 3)
    unique_colors, counts = np.unique(pixels, axis=0, return_counts=True)
    most_common_color = unique_colors[np.argmax(counts)]

    # Convert RGB to the closest color name
    try:
        color_name = webcolors.rgb_to_name(tuple(most_common_color))
    except ValueError:
        # If RGB is not found in the webcolors, return a fallback name
        color_name = "Unknown"

    return color_name

def segment_objects(car_model_path, parkinglot_model_path, input_video_path, output_video_path, iou_threshold=0.50, overlap_threshold=12, apply_illegal_parking=True):
    # Load YOLO models
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    car_model = YOLO(car_model_path)
    car_model.to(device)

    parkinglot_model = YOLO(parkinglot_model_path)
    parkinglot_model.to(device)

    number_plate_model = YOLO(os.path.join("weights", "21_January_25_weights.pt"))
    number_plate_model.to(device)

    cap = cv2.VideoCapture(input_video_path)
    if not cap.isOpened():
        print("Error: Could not open video.")
        sys.exit()

    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))

    # Initialize DeepSort tracker for cars
    tracker = DeepSort(max_age=300, nms_max_overlap=0.4, max_cosine_distance=0.1, max_iou_distance=0.7)

    # Initialize PaddleOCR
    ocr = PaddleOCR(use_angle_cls=True, lang='en')

    detection_data = {}
    parking_status = "UNKNOWN"
    
    frame_num = 0  

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Detect Number Plate 
        number_plate_results = number_plate_model(frame, iou=iou_threshold, device=device)
        extracted_text = ""

        for box, cls, score in zip(number_plate_results[0].boxes.xyxy.cpu().numpy(),
                                    number_plate_results[0].boxes.cls.cpu().numpy(),
                                    number_plate_results[0].boxes.conf.cpu().numpy()):
            if number_plate_model.names[int(cls)] == "Number Plate" and score > 0.5:
                x1, y1, x2, y2 = map(int, box)
                roi = frame[y1:y2, x1:x2]  # Crop number plate region
                
                if roi.size != 0:
                    gray_plate = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)  # Convert to grayscale
                    
                    # Perform OCR with PaddleOCR
                    ocr_results = ocr.ocr(gray_plate, cls=True)
                    extracted_texts = [line[1][0] for res in ocr_results if res for line in res]
                    extracted_text = extracted_texts[0] if extracted_texts else ""

                    # Draw OCR text on frame
                    if extracted_text:
                        cv2.putText(frame, extracted_text, (x1, y2 + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

        # Process parking lot segmentation
        parkinglot_results = parkinglot_model(frame, iou=iou_threshold, device=device)
        parkinglot_masks = []
        if parkinglot_results[0].masks is not None:
            parkinglot_mask_arrays = parkinglot_results[0].masks.data.cpu().numpy()
            parkinglot_classes = parkinglot_results[0].boxes.cls.cpu().numpy()
            parkinglot_scores = parkinglot_results[0].boxes.conf.cpu().numpy()

            for mask, cls, score in zip(parkinglot_mask_arrays, parkinglot_classes, parkinglot_scores):
                class_name = parkinglot_model.names[int(cls)]
                if class_name == "parkinglot":
                    binary_mask = (mask > 0.5).astype("uint8")
                    resized_mask = cv2.resize(binary_mask, (frame_width, frame_height), interpolation=cv2.INTER_LINEAR)
                    parkinglot_masks.append(resized_mask)

        # Process car detection with DeepSort tracking
        car_results = car_model(frame, iou=iou_threshold, device=device)
        if car_results[0].boxes is not None:
            car_boxes = car_results[0].boxes.xyxy.cpu().numpy()
            car_classes = car_results[0].boxes.cls.cpu().numpy()
            car_scores = car_results[0].boxes.conf.cpu().numpy()

            bbs = []  # List to store car bounding boxes for tracking
            for box, cls, score in zip(car_boxes, car_classes, car_scores):
                class_name = car_model.names[int(cls)]
                if class_name == "car" and score > 0.4:
                    overlap_count = is_car_in_multiple_parking_lots(box, parkinglot_masks, threshold=overlap_threshold)
                    x1, y1, x2, y2 = map(int, box)

                    # Apply Illegal Parking logic
                    if apply_illegal_parking:
                        if overlap_count >= 2:
                            parking_status = "ILLEGAL"
                        elif overlap_count == 1:
                            parking_status = "LEGAL"
                        else:
                            parking_status = "UNKNOWN"

                    # Prepare bounding boxes for DeepSort tracking
                    bbs.append(([x1, y1, x2 - x1, y2 - y1], score, class_name))

            if bbs:
                tracks = tracker.update_tracks(bbs, frame=frame) 
            else:
                tracks = tracker.update_tracks([], frame=frame)  

            for track in tracks:
                if not track.is_confirmed():
                    continue

                track_id = track.track_id  
                ltrb = track.to_ltrb()
                x1, y1, x2, y2 = map(int, ltrb)

                # Initialize or update detection data for the car
                if track_id not in detection_data:
                    detection_data[track_id] = {
                        "label": "car",
                        "first_detected": frame_num / fps,
                        "total_time": 0,
                        "illegal_time": 0,  
                        "is_illegal": False,  
                        "ocr_text": extracted_text,
                    }

                # Track illegal parking time
                if parking_status == "ILLEGAL":
                    detection_data[track_id]["illegal_time"] += 1  

                detection_data[track_id]["total_time"] = frame_num / fps - detection_data[track_id]["first_detected"]

                # Determine final parking status
                if detection_data[track_id]["illegal_time"] > detection_data[track_id]["total_time"] / 2:
                    detection_data[track_id]["is_illegal"] = True
                    detection_data[track_id]["final_parking_status"] = "ILLEGAL"
                elif not detection_data[track_id]["is_illegal"]:
                    detection_data[track_id]["final_parking_status"] = "LEGAL"

                # Draw labels on frame
                cv2.putText(frame, f"ID: {track_id}", (x1, y1 + 55), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)
                cv2.putText(frame, f"Status: {detection_data[track_id]['final_parking_status']}", (x1, y1 + 80), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 255), 2)

        # Write the processed frame to the output video
        out.write(frame)

        frame_num += 1  

    # Release video resources
    cap.release()
    out.release()
    cv2.destroyAllWindows()

    # Generate the report
    report_path = generate_report(detection_data, input_video_path, output_video_path)

    return output_video_path



def format_elapsed_time(elapsed_seconds):
    return time.strftime("%H:%M:%S", time.gmtime(elapsed_seconds))


def format_timestamp(timestamp):
    if isinstance(timestamp, datetime):
        timestamp = timestamp.timestamp()  # Convert datetime to Unix timestamp
    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))


def process_video(video_path, weights_path, output_path, selected_labels):
    # Ensure correct device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load YOLO model
    model = YOLO(weights_path)
    model.to(device)

    # Initialize DeepSORT tracker
    tracker = DeepSort(
        max_age=300,
        nms_max_overlap=1.0,
        max_cosine_distance=0.2,
        max_iou_distance=0.7,
    )

    cap = cv2.VideoCapture(video_path)

    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')

    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

    # Initialize PaddleOCR
    ocr = PaddleOCR(use_angle_cls=True, lang='en')

    detection_data = {}  
    previous_position = {}

    frame_num = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_num += 1

        # Run YOLO detection
        results = model.predict(frame, conf=0.40, verbose=False, device=device)
        detections = results[0]

        bbs = []  
        extracted_texts = {}

        for box in detections.boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            label = model.names[int(box.cls)]  # Get actual detected label
            confidence = box.conf[0].item()

            if label in selected_labels and confidence > 0.4:
                bbs.append(([x1, y1, x2 - x1, y2 - y1], confidence, label))

                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

                # Extract text if it's a number plate
                if label == "Number Plate":
                    plate_roi = frame[y1:y2, x1:x2]
                    if plate_roi.size != 0:
                        # Convert to grayscale for better OCR accuracy
                        gray_plate = cv2.cvtColor(plate_roi, cv2.COLOR_BGR2GRAY)
                        
                        # Perform OCR
                        ocr_results = ocr.ocr(gray_plate, cls=True)
                        extracted_text_list = [line[1][0] for res in ocr_results if res for line in res]
                        extracted_text = extracted_text_list[0] if extracted_text_list else ""
                        extracted_texts[(x1, y1, x2, y2)] = extracted_text

                        # Draw OCR text on frame
                        if extracted_text:
                            cv2.putText(frame, extracted_text, (x1, y2 + 30), 
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

        # Update DeepSORT tracker
        tracks = tracker.update_tracks(bbs, frame=frame) if bbs else tracker.update_tracks([], frame=frame)

        for track in tracks:
            if not track.is_confirmed():
                continue

            track_id = track.track_id
            ltrb = track.to_ltrb()
            x1, y1, x2, y2 = map(int, ltrb)

            # Ensure label is stored correctly
            if track_id not in detection_data:
                detection_data[track_id] = {
                    "label": None,  # Initialize label, will be updated below
                    "first_detected": frame_num / fps,
                    "total_time": 0,
                    "ocr_text": ""
                }

            # Get the correct label from the detected objects
            for box, lbl in zip(detections.boxes.xyxy.cpu().numpy(), detections.boxes.cls.cpu().numpy()):
                bx1, by1, bx2, by2 = map(int, box)
                if abs(x1 - bx1) < 20 and abs(y1 - by1) < 20:  # Match detection with tracking
                    detection_data[track_id]["label"] = model.names[int(lbl)]
                    break

            # Update total time tracked
            detection_data[track_id]["total_time"] = (frame_num / fps) - detection_data[track_id]["first_detected"]

            # Assign OCR text if it's a number plate
            if detection_data[track_id]["label"] == "Number Plate" and (x1, y1, x2, y2) in extracted_texts:
                detection_data[track_id]["ocr_text"] = extracted_texts[(x1, y1, x2, y2)]

            # Prevent false jumps in tracking
            if track_id in previous_position:
                prev_x1, prev_y1, prev_x2, prev_y2 = previous_position[track_id]
                distance = ((x1 - prev_x1) ** 2 + (y1 - prev_y1) ** 2) ** 0.5
                if distance > 100:
                    continue

            previous_position[track_id] = (x1, y1, x2, y2)

            # Draw label and ID on the frame
            actual_label = detection_data[track_id]["label"] or "Unknown"
            label_text = f"{actual_label}, ID: {track_id}"
            cv2.putText(frame, label_text, (x1, y1 - 10), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

        out.write(frame)

    cap.release()
    out.release()

    report_path = generate_report(detection_data, video_path, output_path)

    return output_path, report_path


def generate_report(detection_data, video_path, output_path):
    
    input_filename = os.path.basename(video_path)
    input_name, _ = os.path.splitext(input_filename)
    output_filename = os.path.basename(output_path)

    file_stats = os.stat(video_path)
    creation_time = datetime.fromtimestamp(file_stats.st_mtime)  
    modification_time = datetime.fromtimestamp(file_stats.st_ctime)  

    formatted_creation_time = format_timestamp(creation_time)
    formatted_modification_time = format_timestamp(modification_time)

    report_path = output_path.replace(".mp4", "_report.csv")

    with open(report_path, mode="w", newline="") as file:
        fieldnames = ["Event ID", "Label", "OCR Text", "First Detected", "Total Time", "Car Color", "Parking Status"]

        writer = csv.DictWriter(file, fieldnames=fieldnames)

        file.write(f"# Input File: {input_filename}\n")
        file.write(f"# Output File: {output_filename}\n")
        file.write(f"# Video File Created: {formatted_creation_time}\n")
        file.write(f"# Video File Modified: {formatted_modification_time}\n")

        writer.writeheader()
        seen_number_plates = set()

        for track_id, data in detection_data.items():
            first_detected_str = format_elapsed_time(data["first_detected"])  
            total_time_str = format_elapsed_time(data["total_time"])

            if data["label"] in ["Number Plate", "car"]:
                ocr_text = data["ocr_text"].strip()
                if data["label"] in ["Number Plate", "car"] and ocr_text in seen_number_plates:
                    ocr_text = ""  
                else:
                    seen_number_plates.add(ocr_text)  
                
                writer.writerow({
                    "Event ID": track_id,
                    "Label": data["label"],
                    "OCR Text": ocr_text, 
                    "First Detected": first_detected_str,
                    "Total Time": total_time_str,
                    "Car Color": data.get("color", ""),  
                    "Parking Status": data.get("final_parking_status", "") 
                })

            else:
                writer.writerow({
                    "Event ID": track_id,
                    "Label": data["label"],
                    "OCR Text": " ",  
                    "First Detected": first_detected_str,
                    "Total Time": total_time_str,
                    "Car Color": data.get("color", ""),  
                    "Parking Status": data.get("final_parking_status", "")  
                })

    print(f"Report saved to: {report_path}")

    return report_path


    
def process_video_and_ocr(video_path, selected_labels):
    start_time = time.time()
    input_filename = os.path.basename(video_path)
    input_name, _ = os.path.splitext(input_filename)
    video_path = os.path.join('videos', video_path)

    weights_dictionary = {
        # "Water Spilled": os.path.join("weights", "21_January_25_weights.pt"),
        "Other": os.path.join("weights", "21_January_25_weights.pt"),
        "Car Detection": os.path.join("weights", "weights_for_car.pt"),
        "ParkingLot": os.path.join("weights", "Angles_parkinglot_new_weights.pt"),
        "New Labels" : os.path.join("weights","Dogs_bin_truck.pt"),
    }

    label_string = "_".join(selected_labels).replace(" ", "_")

    output_video_path = os.path.join("output", f"{input_name}_{label_string}.mp4")

    # Check for Illegal Parking label and process accordingly
    if "Illegal Parking" in selected_labels:
        car_model_path = weights_dictionary["Car Detection"]  
        parkinglot_model_path = weights_dictionary["ParkingLot"]  
        video_path = segment_objects(car_model_path, parkinglot_model_path, video_path, output_video_path)
        
        processing_time = time.time() - start_time
        processing_time_formatted = time.strftime('%H:%M:%S', time.gmtime(processing_time))
    
        return output_video_path, None, processing_time_formatted  
    
    else:
        selected_weights = [weights_dictionary[label] for label in selected_labels if label in weights_dictionary]

        if "Water Spilled" in selected_labels:
            weights_path = weights_dictionary["Other"]
        
        elif "Dogs" in selected_labels or "Garbage Truck" in selected_labels or "Garbage bin" in selected_labels:
            weights_path = weights_dictionary["New Labels"]     
        
        elif not selected_weights:  
            weights_path = weights_dictionary["Other"]

        output_video_path, report_path = process_video(video_path, weights_path, output_video_path, selected_labels)
        
        processing_time = time.time() - start_time
        processing_time_formatted = time.strftime('%H:%M:%S', time.gmtime(processing_time))
    
        return output_video_path, report_path, processing_time_formatted


def list_videos(folder_path):
    """List all video files in the given folder."""
    return [file for file in os.listdir(folder_path) if file.endswith(('.mp4', '.avi', '.mov', '.mkv'))]


def main():
    label_mapping = {
        
        1: "Garbage",
        
        2: "Number Plate",
        
        3: "pedestrian",
        4: "street light",
        5: "Construction Material",
        6: "Water Spilled",
        
        7: "Illegal Parking",
        
        8: "Dogs",
        9: "Garbage Truck",
        10: "Garbage bin"
    
    }
    video_folder = "videos"  
    output_folder = "output"  
    videos = list_videos(video_folder)
    logo_path = "/home/ubuntu/app/exponent.png"

    interface = gr.Interface(
        fn=process_video_and_ocr,
        inputs=[
            gr.Dropdown(videos, label="Select Video"),
            gr.CheckboxGroup(
                choices=list(label_mapping.values()), label="Select Labels to Detect"
            ),
        ],
        outputs=[
            gr.File(label="Download Processed Video"),
            gr.File(label="Download CSV Report"),
            gr.Text(label="Processing Time (HH:MM:SS)")
        ],
        title="RCS VISION AI",
        description="Select a video and labels for detection. Detect objects and generate a CSV report with detection times.",
        allow_flagging="never",
    )

    with gr.Blocks() as app:
        gr.Image(logo_path, elem_id="logo")

        interface.render()

    app.launch(share=True, debug=True)

if __name__ == "__main__":
    main()